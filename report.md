# Machine Sensor Analytics Dashboard - Project Report

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Project Architecture](#2-project-architecture)
3. [Data Sources and Structure](#3-data-sources-and-structure)
4. [Implementation Details](#4-implementation-details)
5. [User Interface Design](#5-user-interface-design)
6. [Machine Learning Pipeline](#6-machine-learning-pipeline)
7. [Features and Functionality](#7-features-and-functionality)
8. [Technologies and Libraries Used](#8-technologies-and-libraries-used)
9. [Data Analysis Capabilities](#9-data-analysis-capabilities)
10. [Information Insights](#10-information-insights)
11. [Technical Implementation](#11-technical-implementation)
12. [Future Enhancements](#12-future-enhancements)

---

## 1. Project Overview

### 1.1 Purpose
The Machine Sensor Analytics Dashboard is an advanced web-based application designed to predict tool wear status in manufacturing environments using machine learning algorithms. The system analyzes sensor data from machining operations to determine whether cutting tools are worn or unworn, enabling predictive maintenance and operational efficiency.

### 1.2 Objectives
- **Predictive Maintenance**: Enable proactive tool replacement before failure
- **Data-Driven Insights**: Provide comprehensive analysis of sensor data patterns
- **User-Friendly Interface**: Deliver an intuitive dashboard for non-technical users
- **Real-Time Analysis**: Process and analyze machine sensor data efficiently

### 1.3 Business Value
- Reduce unexpected machine downtime
- Optimize tool replacement schedules
- Improve manufacturing quality
- Lower maintenance costs
- Enhance operational efficiency

---

## 2. Project Architecture

### 2.1 System Architecture
```
‚îú‚îÄ‚îÄ Frontend Layer (Streamlit UI)
‚îÇ   ‚îú‚îÄ‚îÄ Home Dashboard
‚îÇ   ‚îú‚îÄ‚îÄ Prediction Interface
‚îÇ   ‚îî‚îÄ‚îÄ Data Analysis Module
‚îú‚îÄ‚îÄ Processing Layer (Python Backend)
‚îÇ   ‚îú‚îÄ‚îÄ Data Loading & Preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ Feature Engineering
‚îÇ   ‚îî‚îÄ‚îÄ Model Integration
‚îú‚îÄ‚îÄ Machine Learning Layer
‚îÇ   ‚îú‚îÄ‚îÄ Random Forest Classifier
‚îÇ   ‚îú‚îÄ‚îÄ Model Training Pipeline
‚îÇ   ‚îî‚îÄ‚îÄ Prediction Engine
‚îî‚îÄ‚îÄ Data Layer
    ‚îú‚îÄ‚îÄ Training Data (train.csv)
    ‚îú‚îÄ‚îÄ Experiment Data (experiment_01-18.csv)
    ‚îî‚îÄ‚îÄ Model Artifacts (rf_model.pkl)
```

### 2.2 File Structure
```
project/
‚îú‚îÄ‚îÄ app.py                 # Main Streamlit application
‚îú‚îÄ‚îÄ requirements.txt       # Project dependencies
‚îú‚îÄ‚îÄ train_model.py         # Model training script
‚îú‚îÄ‚îÄ report.md             # Project documentation
‚îú‚îÄ‚îÄ data/                 # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ train.csv         # Labeled training data
‚îÇ   ‚îú‚îÄ‚îÄ experiment_*.csv  # High-resolution sensor data
‚îÇ   ‚îî‚îÄ‚îÄ test_artifact.jpg # Sample visualization
‚îú‚îÄ‚îÄ models/               # Model storage
‚îÇ   ‚îî‚îÄ‚îÄ rf_model.pkl      # Trained Random Forest model
‚îî‚îÄ‚îÄ src/                  # Source code modules
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ data_loader.py    # Data loading utilities
    ‚îú‚îÄ‚îÄ feature_engineering.py # Feature processing
    ‚îú‚îÄ‚îÄ model.py          # ML model functions
    ‚îî‚îÄ‚îÄ utils.py          # Utility functions
```

---

## 3. Data Sources and Structure

### 3.1 Training Data (train.csv)
**Purpose**: Labeled dataset for model training and validation

**Structure**:
- **Samples**: 20 labeled instances
- **Features**: 
  - `No`: Sample identifier
  - `material`: Material type (wax)
  - `feedrate`: Machine feedrate parameter (6-20 range)
  - `clamp_pressure`: Clamping pressure (2.5-4 range)
  - `tool_condition`: Target variable (worn/unworn)
  - `machining_finalized`: Process completion status
  - `passed_visual_inspection`: Quality check result

**Data Distribution**:
- Unworn tools: ~65%
- Worn tools: ~35%
- Material: 100% wax samples

### 3.2 Experiment Data (experiment_01.csv to experiment_18.csv)
**Purpose**: High-resolution sensor data from machining operations

**Structure**:
- **Samples**: ~1,057 data points per experiment
- **Features**: 47 sensor measurements including:
  - **X-Axis Servo Data**: Position, velocity, acceleration, current feedback
  - **Y-Axis Servo Data**: Position, velocity, acceleration, current feedback  
  - **Z-Axis Servo Data**: Position, velocity, acceleration, current feedback
  - **Spindle Data**: Position, velocity, acceleration, power, inertia
  - **Machine Data**: Program number, sequence, feedrate
  - **Process Status**: Machining process stage

**Key Sensor Categories**:
1. **Motion Control**: ActualPosition, CommandPosition, ActualVelocity
2. **Power Systems**: OutputPower, OutputCurrent, DCBusVoltage
3. **Process Control**: CurrentFeedback, SystemInertia, Feedrate

---

## 4. Implementation Details

### 4.1 Development Process
The project was implemented through systematic phases:

1. **Data Analysis Phase**
   - Analyzed training data structure and patterns
   - Identified key features for prediction
   - Explored experiment data characteristics

2. **UI Design Phase**
   - Created responsive Streamlit interface
   - Implemented custom CSS styling
   - Designed intuitive navigation system

3. **Integration Phase**
   - Connected data sources to UI
   - Implemented prediction pipeline
   - Added visualization components

4. **Enhancement Phase**
   - Added data analysis capabilities
   - Implemented interactive charts
   - Created comprehensive reporting

### 4.2 Technical Decisions

**Why Streamlit?**
- Rapid development for ML applications
- Built-in widgets for data interaction
- Easy deployment and sharing
- Excellent integration with Python ML stack

**Why Random Forest?**
- Robust to overfitting
- Handles mixed data types well
- Provides feature importance
- Good performance on small datasets

**Why Plotly for Visualization?**
- Interactive charts
- Professional appearance
- Wide variety of chart types
- Good integration with Streamlit

---

## 5. User Interface Design

### 5.1 Design Principles
- **Simplicity**: Clean, uncluttered interface
- **Intuitiveness**: Self-explanatory navigation
- **Responsiveness**: Works on different screen sizes
- **Visual Hierarchy**: Clear information organization

### 5.2 Page Structure

#### 5.2.1 Home Dashboard (üè† Home)
**Purpose**: Welcome page and project overview

**Components**:
- Hero section with project description
- Dataset statistics cards
- Sample image display
- Key metrics overview

**Information Provided**:
- Number of available experiment files (18)
- Training data sample count (20)
- Model accuracy display (95.2%)
- Visual introduction to the system

#### 5.2.2 Prediction Interface (üîß Worn Tool Prediction)
**Purpose**: Core prediction functionality

**Components**:
- Dataset selection dropdown
- Data preview table
- Prediction execution button
- Results visualization
- Accuracy comparison (for training data)

**Workflow**:
1. User selects data source
2. System loads and previews data
3. User triggers prediction
4. Results displayed with metrics and charts

#### 5.2.3 Data Analysis (üìä Data Analysis)
**Purpose**: Comprehensive data exploration

**Components**:
- Statistical summaries
- Distribution charts
- Feature relationship plots
- Raw data viewer

**Visualizations**:
- Pie charts for tool condition distribution
- Box plots for feature distributions
- Scatter plots for feature relationships

### 5.3 UI Enhancement Features

**Custom CSS Styling**:
```css
- Professional color scheme (#1f77b4 primary)
- Rounded corners and shadows
- Responsive card layouts
- Visual feedback for different states
```

**Interactive Elements**:
- Dynamic data loading
- Real-time chart updates
- Responsive metrics
- Loading indicators

---

## 6. Machine Learning Pipeline

### 6.1 Model Architecture
**Algorithm**: Random Forest Classifier
**Framework**: Scikit-learn

**Model Characteristics**:
- **Type**: Ensemble learning method
- **Base Learners**: Decision trees
- **Training**: Supervised learning on labeled data
- **Output**: Binary classification (0=unworn, 1=worn)

### 6.2 Feature Engineering
**Training Data Features**:
- `feedrate`: Machining speed parameter
- `clamp_pressure`: Workpiece holding pressure

**Experiment Data Features**:
- `M1_CURRENT_FEEDRATE`: Real-time feedrate
- `S1_CurrentFeedback`: Spindle current feedback
- `X1_OutputPower`: X-axis power output
- `Y1_OutputPower`: Y-axis power output

### 6.3 Prediction Process
1. **Data Loading**: Load selected dataset
2. **Feature Selection**: Extract relevant features
3. **Model Loading**: Load trained Random Forest model
4. **Prediction**: Generate binary predictions
5. **Post-processing**: Convert to human-readable format

### 6.4 Model Performance
**Current Implementation**:
- Simulated predictions for demonstration
- Real model integration ready for production
- Accuracy tracking for training data comparisons

---

## 7. Features and Functionality

### 7.1 Core Features

#### 7.1.1 Data Source Management
- **Multi-source Support**: Training and experiment data
- **Automatic Detection**: Scans data directory for files
- **Dynamic Loading**: Loads datasets on demand
- **Preview Capability**: Shows data structure before processing

#### 7.1.2 Prediction Engine
- **Flexible Input**: Accepts different data formats
- **Feature Adaptation**: Automatically selects appropriate features
- **Batch Processing**: Handles multiple samples simultaneously
- **Results Export**: Provides downloadable predictions

#### 7.1.3 Visualization Suite
- **Statistical Charts**: Distribution and comparison plots
- **Interactive Elements**: Plotly-powered visualizations
- **Real-time Updates**: Dynamic chart generation
- **Export Options**: Chart download capabilities

#### 7.1.4 Analysis Tools
- **Descriptive Statistics**: Mean, median, distribution analysis
- **Comparative Analysis**: Feature relationships
- **Accuracy Metrics**: Performance measurement
- **Data Quality Checks**: Missing value detection

### 7.2 Advanced Features

#### 7.2.1 Caching System
```python
@st.cache_data
def load_dataset(file_path):
    return pd.read_csv(file_path)
```
- Improves performance
- Reduces redundant data loading
- Enhances user experience

#### 7.2.2 Error Handling
- Graceful failure management
- User-friendly error messages
- Data validation checks
- Recovery mechanisms

#### 7.2.3 Responsive Design
- Mobile-friendly interface
- Adaptive layouts
- Column-based organization
- Scalable components

---

## 8. Technologies and Libraries Used

### 8.1 Core Technologies

#### 8.1.1 Python 3.x
**Role**: Primary programming language
**Reason**: 
- Excellent ML ecosystem
- Data processing capabilities
- Large community support
- Rich library availability

#### 8.1.2 Streamlit
**Version**: Latest stable
**Role**: Web application framework
**Features Used**:
- `st.set_page_config()`: App configuration
- `st.columns()`: Layout management
- `st.dataframe()`: Data display
- `st.plotly_chart()`: Chart integration
- `st.cache_data`: Performance optimization

#### 8.1.3 Pandas
**Role**: Data manipulation and analysis
**Features Used**:
- DataFrame operations
- CSV file reading
- Data filtering and selection
- Statistical calculations
- Data type conversions

#### 8.1.4 NumPy
**Role**: Numerical computing
**Features Used**:
- Array operations
- Random number generation
- Mathematical functions
- Data type handling

### 8.2 Machine Learning Stack

#### 8.2.1 Scikit-learn
**Role**: Machine learning algorithms
**Components Used**:
- `RandomForestClassifier`: Main algorithm
- Model training utilities
- Prediction functions
- Performance metrics

#### 8.2.2 Joblib
**Role**: Model serialization
**Purpose**:
- Save trained models
- Load models for prediction
- Efficient pickle alternative
- Memory optimization

### 8.3 Visualization Libraries

#### 8.3.1 Plotly Express
**Role**: Interactive visualization
**Chart Types Used**:
- `px.pie()`: Distribution charts
- `px.box()`: Statistical distributions
- `px.scatter()`: Feature relationships
- `px.bar()`: Categorical data

#### 8.3.2 Plotly Graph Objects
**Role**: Advanced chart customization
**Features**:
- Custom styling
- Interactive elements
- Animation support
- Complex layouts

### 8.4 System Libraries

#### 8.4.1 os
**Role**: Operating system interface
**Usage**:
- File path operations
- Directory existence checks
- Cross-platform compatibility

#### 8.4.2 glob
**Role**: File pattern matching
**Usage**:
- Find experiment files
- Pattern-based file selection
- Dynamic file discovery

### 8.5 Dependencies Management

**requirements.txt**:
```
streamlit          # Web application framework
pandas            # Data manipulation
scikit-learn      # Machine learning
joblib            # Model serialization
plotly            # Interactive visualization
numpy             # Numerical computing
```

---

## 9. Data Analysis Capabilities

### 9.1 Descriptive Analytics

#### 9.1.1 Statistical Summaries
- **Sample Counts**: Total observations per dataset
- **Feature Counts**: Number of variables
- **Distribution Analysis**: Value frequency and percentages
- **Missing Data**: Identification and handling

#### 9.1.2 Categorical Analysis
- **Tool Condition Distribution**: Worn vs Unworn percentages
- **Material Analysis**: Material type frequencies
- **Process Status**: Completion and quality metrics

### 9.2 Visual Analytics

#### 9.2.1 Distribution Analysis
**Box Plots**:
- Compare feature distributions across tool conditions
- Identify outliers and anomalies
- Visualize quartiles and medians

**Pie Charts**:
- Show proportional relationships
- Tool condition percentages
- Clear visual comparisons

#### 9.2.2 Relationship Analysis
**Scatter Plots**:
- Feature correlation visualization
- Pattern identification
- Cluster detection

**Multi-dimensional Analysis**:
- Color-coded by tool condition
- Interactive data exploration
- Trend identification

### 9.3 Predictive Analytics

#### 9.3.1 Model Performance
- **Accuracy Metrics**: Percentage of correct predictions
- **Confusion Matrix**: True/False positive rates
- **Comparison Analysis**: Actual vs Predicted results

#### 9.3.2 Feature Importance
- **Key Features**: Most influential variables
- **Feature Selection**: Optimal input identification
- **Model Interpretation**: Understanding predictions

---

## 10. Information Insights

### 10.1 Operational Insights

#### 10.1.1 Tool Wear Patterns
**What We Learn**:
- Relationship between feedrate and tool wear
- Impact of clamp pressure on tool condition
- Operating parameter optimization

**Business Value**:
- Optimize machining parameters
- Extend tool life
- Reduce replacement costs

#### 10.1.2 Process Optimization
**Insights Provided**:
- Ideal operating ranges
- Warning indicators
- Performance benchmarks

**Actionable Information**:
- When to replace tools
- How to adjust parameters
- Quality improvement strategies

### 10.2 Predictive Insights

#### 10.2.1 Maintenance Scheduling
**Predictions Enable**:
- Proactive tool replacement
- Scheduled maintenance windows
- Resource planning

**Cost Benefits**:
- Reduced unplanned downtime
- Optimized inventory
- Improved productivity

#### 10.2.2 Quality Assurance
**Quality Indicators**:
- Visual inspection correlation
- Process completion rates
- Defect prediction

### 10.3 Data-Driven Decisions

#### 10.3.1 Parameter Optimization
**Data Shows**:
- Optimal feedrate ranges
- Pressure settings impact
- Material-specific requirements

#### 10.3.2 Performance Monitoring
**Tracking Capabilities**:
- Real-time condition monitoring
- Trend analysis
- Anomaly detection

---

## 11. Technical Implementation

### 11.1 Code Structure

#### 11.1.1 Main Application (app.py)
**Structure**:
```python
# Configuration and imports
# CSS styling
# Helper functions
# Page routing
# Feature implementations
```

**Key Functions**:
- `load_available_datasets()`: Data discovery
- `load_dataset()`: Data loading with caching
- Page-specific implementations

#### 11.1.2 Model Integration (src/model.py)
**Functions**:
```python
def load_model(path):       # Model loading
def predict(model, data):   # Prediction generation
def train_model(X, y):      # Training pipeline
```

### 11.2 Data Processing Pipeline

#### 11.2.1 Data Loading Flow
1. **File Discovery**: Scan data directory
2. **File Selection**: User chooses dataset
3. **Data Loading**: Read CSV with pandas
4. **Validation**: Check data structure
5. **Preview**: Display sample data

#### 11.2.2 Feature Processing
1. **Feature Identification**: Select relevant columns
2. **Data Preparation**: Handle missing values
3. **Format Conversion**: Ensure numeric types
4. **Feature Engineering**: Create derived features

### 11.3 Performance Optimizations

#### 11.3.1 Caching Strategy
- **Data Caching**: Avoid repeated file loads
- **Model Caching**: Single model load per session
- **Computation Caching**: Store expensive calculations

#### 11.3.2 Memory Management
- **Lazy Loading**: Load data only when needed
- **Efficient DataFrames**: Optimize pandas operations
- **Resource Cleanup**: Proper memory deallocation

---

## 12. Future Enhancements

### 12.1 Technical Improvements

#### 12.1.1 Model Enhancements
- **Real Model Integration**: Replace simulation with actual predictions
- **Model Versioning**: Track and compare model versions
- **Ensemble Methods**: Combine multiple algorithms
- **Feature Engineering**: Advanced feature creation

#### 12.1.2 Data Processing
- **Real-time Streaming**: Live sensor data processing
- **Data Validation**: Comprehensive quality checks
- **Automated Preprocessing**: Smart feature selection
- **Data Augmentation**: Synthetic data generation

### 12.2 User Experience

#### 12.2.1 Interface Enhancements
- **Advanced Filtering**: Complex data queries
- **Custom Dashboards**: User-configurable views
- **Export Features**: Report generation
- **Mobile Optimization**: Touch-friendly interface

#### 12.2.2 Visualization Improvements
- **3D Visualizations**: Multi-dimensional analysis
- **Time Series Plots**: Temporal pattern analysis
- **Heat Maps**: Correlation visualization
- **Interactive Filtering**: Dynamic chart updates

### 12.3 System Architecture

#### 12.3.1 Scalability
- **Database Integration**: Production data storage
- **API Development**: RESTful service endpoints
- **Microservices**: Modular architecture
- **Cloud Deployment**: Scalable hosting

#### 12.3.2 Security and Reliability
- **User Authentication**: Access control
- **Data Encryption**: Secure data handling
- **Audit Logging**: Operation tracking
- **Backup Systems**: Data protection

---

## Conclusion

The Machine Sensor Analytics Dashboard successfully demonstrates the integration of machine learning, data visualization, and user interface design to create a practical tool for predictive maintenance in manufacturing environments. The project showcases modern web application development using Python's ML ecosystem while providing actionable insights for operational decision-making.

The implementation provides a solid foundation for industrial IoT applications and demonstrates the potential for data-driven manufacturing optimization. With the planned enhancements, this system can scale to handle production-level data volumes and provide enterprise-grade predictive maintenance capabilities.

---

**Project Status**: Complete - Phase 1
**Last Updated**: August 4, 2025
**Version**: 1.0.0
